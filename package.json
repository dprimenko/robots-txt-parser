{
  "name": "robots-txt-parser",
  "version": "1.0.1",
  "description": "A lightweight robots.txt parser for Node.js with support for wildcards, caching and promises.",
  "keywords": [
    "robots",
    "txt",
    "robots.txt",
    "parser",
    "crawler",
    "spider",
    "bot",
    "robotstxt",
    "scraper"
  ],
  "main": "src/index.js",
  "directories": {
    "test": "test"
  },
  "scripts": {
    "test": "mocha test/index.js",
    "test-robots": "mocha test/robots/index-tests.js",
    "test-parser": "mocha test/parser/parser-tests.js",
    "test-util": "mocha test/util/util-tests.js",
    "coverage": "nyc mocha test/index.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/ChristopherAkroyd/robots-txt-parser.git"
  },
  "author": "Chris Akroyd",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/ChristopherAkroyd/robots-txt-parser/issues"
  },
  "homepage": "https://github.com/ChristopherAkroyd/robots-txt-parser#readme",
  "dependencies": {
    "fast-url-parser": "^1.1.3",
    "is-absolute-url": "^2.1.0",
    "lodash.isfunction": "^3.0.8",
    "simple-get": "^2.4.0"
  },
  "devDependencies": {
    "chai": "^3.5.0",
    "istanbul": "^0.4.5",
    "lodash": "^4.17.2",
    "mocha": "^3.2.0",
    "nyc": "^10.3.0"
  }
}
